{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b827cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# model\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# math\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt, isnan\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "fp = \"G:\\\\My Drive\\\\Darby Work\\\\Ytsma and Dyar 2021 (LOD paper)\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf87bfbc",
   "metadata": {},
   "source": [
    "#### Compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af136e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comps\n",
    "mhc_comps = pd.read_csv(fp+'tables\\\\TableS1_MHC_sample_compositions.csv')\n",
    "lanl_comps = pd.read_csv(fp+'tables\\\\TableS2_LANL_sample_compositions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c18bec",
   "metadata": {},
   "source": [
    "#### Datasets (baseline removal applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f455d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = fp + 'datasets//'\n",
    "\n",
    "# same 205 standards\n",
    "cc_mars = pd.read_csv(dp+'CC_norm3_spectra.csv')\n",
    "cl_mars_cc = pd.read_csv(dp+'CL_Mars_norm3_spectra_CC_matched.csv')\n",
    "cc_mars_unnorm = pd.read_csv(dp+'CC_unnorm_spectra.csv')\n",
    "cl_mars_cc_unnorm = pd.read_csv(dp+'CL_Mars_unnorm_spectra_CC_matched.csv')\n",
    "\n",
    "# same 2607 standards\n",
    "cl_mars = pd.read_csv(dp+'CL_Mars_norm3_spectra_matched.csv')\n",
    "cl_earth = pd.read_csv(dp+'CL_Earth_norm3_spectra_matched.csv')\n",
    "cl_vac = pd.read_csv(dp+'CL_Vacuum_norm3_spectra_matched.csv')\n",
    "cl_mars_unnorm = pd.read_csv(dp+'CL_Mars_unnorm_spectra_matched.csv')\n",
    "cl_earth_unnorm = pd.read_csv(dp+'CL_Earth_unnorm_spectra_matched.csv')\n",
    "cl_vac_unnorm = pd.read_csv(dp+'CL_Vacuum_unnorm_spectra_matched.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f629b2",
   "metadata": {},
   "source": [
    "#### Sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b953a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivities = pd.read_csv(fp+'instrument_sensitivities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fde12",
   "metadata": {},
   "source": [
    "#### Outlier limits\n",
    "Calculated by 1.5*IQR + Q3 on entire MHC dataset or highest natural sample for doped elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ea24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_limits = pd.read_csv('Z:\\\\Millennium Set\\\\NEW_OUTLIER_LIMITS.csv')\n",
    "iqr_outliers = dict(zip(outlier_limits.element, outlier_limits.iqr_q3_outlier_limit))\n",
    "dope_outliers = dict(zip(outlier_limits.element, outlier_limits.highest_natural_for_doped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2afb60",
   "metadata": {},
   "source": [
    "#### Make models per element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f409b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for reading into loops\n",
    "normed_dfs = [cl_mars, cl_earth, cl_vac, cc_mars, cl_mars_cc]\n",
    "unnorm_dfs = [cl_mars_unnorm, cl_earth_unnorm, cl_vac_unnorm, cc_mars_unnorm, cl_mars_cc_unnorm]\n",
    "atms = ['Mars', 'Earth', 'Vacuum', 'Mars', 'Mars']\n",
    "insts = ['CL', 'CL', 'CL', 'CC', 'CL_CC']\n",
    "elements = ['MnO', 'Na2O', 'SiO2', 'Li', 'Ni', 'Pb', 'Rb', 'Sr', 'Zn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441a23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020a238712f24dd2b055bbb17a0b2558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f5b03d22e545ce8456333951ac5e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "n_range:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3882e4700eb34ff79df5332396373acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "element:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "outlier:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "outlier:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5a881260fd455087fe97c8bd47c083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "outlier:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLS parameters\n",
    "n_folds = 5\n",
    "max_components = 30\n",
    "\n",
    "# prepare data lists\n",
    "norm_list = []\n",
    "atm_list = []\n",
    "inst_list = []\n",
    "n_range_list = []\n",
    "element_list = []\n",
    "outlier_list = []\n",
    "sens_list = []\n",
    "n_train_list = []\n",
    "rmsecv_list = []\n",
    "component_list = []\n",
    "loq_list = []\n",
    "\n",
    "#for norm in tqdm(['unnorm', 'norm3'], desc='norm'):\n",
    "    \n",
    "    #datasets=unnorm_dfs if norm=='unnorm' else normed_dfs # TEMP CHANGE FROM FIXING ERROR\n",
    "norm = 'norm3'\n",
    "datasets=normed_dfs\n",
    "count=0\n",
    "\n",
    "for dataset in tqdm(datasets, desc='dataset', leave=False): \n",
    "\n",
    "    atm = atms[count]\n",
    "    inst = insts[count]\n",
    "    count+=1\n",
    "\n",
    "    # select correct composition file\n",
    "    comps=lanl_comps if 'CC' in inst else mhc_comps\n",
    "\n",
    "    for n_range in tqdm(['0-750', '250-1000'], desc='n_range', leave=False):\n",
    "\n",
    "        # filter dataset\n",
    "        if n_range == '0-750':\n",
    "            all_comps = comps[comps['Random Number'] <= 750].copy(deep=True)\n",
    "        elif n_range == '250-1000':\n",
    "            all_comps = comps[comps['Random Number'] >= 250].copy(deep=True)\n",
    "\n",
    "        for element in tqdm(elements, desc='element', leave=False):\n",
    "\n",
    "            # get column name\n",
    "            elem_col = [i for i in all_comps.columns if element in i][0]\n",
    "\n",
    "            for outlier in tqdm(['iqr_q3', 'high_natl'], desc='outlier', leave=False):\n",
    "\n",
    "                # define path to save results\n",
    "                outpath = \"{}python_models\\\\{}\\\\{}_{}\\\\{}\\\\{}\\\\\".format(fp,norm,inst,atm,n_range,outlier)\n",
    "\n",
    "                out=iqr_outliers if outlier=='iqr_q3' else dope_outliers\n",
    "                out_lim = out[element]\n",
    "\n",
    "                if isnan(out_lim):\n",
    "                    train_comps = all_comps[~all_comps[elem_col].isna()].reset_index(drop=True)[['Sample Name', elem_col]]\n",
    "                else:\n",
    "                    train_comps = all_comps[all_comps[elem_col] <= out_lim].reset_index(drop=True)[['Sample Name', elem_col]]\n",
    "\n",
    "                # prepare data for modelling\n",
    "                y_train = train_comps[elem_col].values\n",
    "                n_train = len(y_train)\n",
    "\n",
    "                if n_train < n_folds:\n",
    "                    n_folds = n_train\n",
    "\n",
    "                X_train = dataset[list(train_comps['Sample Name'])]\n",
    "                spec_list = []\n",
    "                for column in X_train.columns:\n",
    "                    spectrum = list(X_train[column])\n",
    "                    spec_list.append(spectrum)\n",
    "                X_train = np.array(spec_list)\n",
    "\n",
    "                # cross validation and model training\n",
    "                cv_dict = {}\n",
    "                for n_components in np.arange(start=2, stop=max_components+1, step=1):\n",
    "                    temp_pls = PLSRegression(n_components = n_components, scale=False)\n",
    "                    temp_rmsecv = (-cross_val_score(\n",
    "                        temp_pls, X_train, y_train, cv=n_folds, scoring='neg_root_mean_squared_error'\n",
    "                    )).mean()\n",
    "                    cv_dict.update({temp_rmsecv : n_components})\n",
    "\n",
    "                # select parameters of model with lowest rmsecv\n",
    "                rmsecv = min(list(cv_dict.keys()))\n",
    "                component = cv_dict[rmsecv]\n",
    "                model = PLSRegression(n_components = component, scale=False)\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                pickle.dump(model, open(outpath+element+'_model.asc', 'wb'), protocol=0)\n",
    "\n",
    "                coeff = pd.DataFrame(model.coef_)\n",
    "                coeff.to_csv(outpath+element+'_coeffs.csv', index=False)\n",
    "\n",
    "                for sens in ['braga', 'metals']:\n",
    "\n",
    "                    if inst == 'CL_CC':\n",
    "                        sensitivity = sensitivities[\n",
    "                            (sensitivities.instrument == 'CL') &\n",
    "                            (sensitivities.atmosphere == atm) &\n",
    "                            (sensitivities.normalization == norm) &\n",
    "                            (sensitivities.method == sens)\n",
    "                        ]['sensitivity'].iloc[0]\n",
    "                    else:\n",
    "                        sensitivity = sensitivities[\n",
    "                            (sensitivities.instrument == inst) &\n",
    "                            (sensitivities.atmosphere == atm) &\n",
    "                            (sensitivities.normalization == norm) &\n",
    "                            (sensitivities.method == sens)\n",
    "                        ]['sensitivity'].iloc[0]                            \n",
    "\n",
    "                    # calculate LOQ\n",
    "                    vector = pow(coeff, 2).sum().pow(.5)\n",
    "                    loq = 10 * sensitivity * vector[0]\n",
    "\n",
    "                    # descriptive\n",
    "                    norm_list.append(norm)\n",
    "                    atm_list.append(atm)\n",
    "                    inst_list.append(inst)\n",
    "                    n_range_list.append(n_range)\n",
    "                    element_list.append(element)\n",
    "                    outlier_list.append(outlier)\n",
    "                    sens_list.append(sens)\n",
    "                    # unique values\n",
    "                    n_train_list.append(n_train)\n",
    "                    rmsecv_list.append(rmsecv)\n",
    "                    component_list.append(component)\n",
    "                    loq_list.append(loq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ea465",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'element':element_list,\n",
    "    'instrument':inst_list,\n",
    "    'atmosphere':atm_list,\n",
    "    'normalization':norm_list,\n",
    "    'outlier_defn':outlier_list,\n",
    "    'sens_method':sens_list,\n",
    "    'num_range':n_range_list,\n",
    "    'n_train':n_train_list,\n",
    "    'rmsecv':rmsecv_list,\n",
    "    'components':component_list,\n",
    "    'loq':loq_list\n",
    "})\n",
    "\n",
    "results.to_csv(fp+'train_results_020422.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results.groupby(['element', \n",
    "                     'normalization',\n",
    "                     'outlier_defn', \n",
    "                     'atmosphere',\n",
    "                     'instrument',\n",
    "                     'sens_method'], as_index=False)\n",
    "\n",
    "avg = r.mean()\n",
    "std = r.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcols = list(std.columns[:6])\n",
    "sd_list = [i + '_sd' for i in std.columns[6:]]\n",
    "for i in [0,1,2,3,4,5]:\n",
    "    sd_list.insert(i, fcols[i])\n",
    "std.columns = sd_list\n",
    "avg_results = pd.merge(avg, std)\n",
    "avg_results.to_csv(fp+'average_train_results_020422.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
