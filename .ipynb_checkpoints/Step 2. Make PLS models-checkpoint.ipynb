{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b827cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# model\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# math\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "fp = \"G:\\\\My Drive\\\\Darby Work\\\\Ytsma and Dyar 2021 (LOD paper)\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf87bfbc",
   "metadata": {},
   "source": [
    "#### Compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af136e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate comps\n",
    "comps_path = fp + \"tables\\\\TableS1_sample_compositions.xlsx\"\n",
    "lanl_comps = pd.read_excel(comps_path, sheet_name = \"LANL\")\n",
    "mhc_comps = pd.read_excel(comps_path, sheet_name = \"MHC\")\n",
    "comps = pd.merge(mhc_comps, lanl_comps, how = \"outer\") # merge comps\n",
    "comps.columns = comps.columns.map(lambda x: x.split()[0])\n",
    "comps = comps.drop_duplicates(subset = 'Sample') # remove duplicates\n",
    "comps['Sample'] = comps['Sample'].astype(str)\n",
    "comps = comps.sort_values(by='Sample')\n",
    "comps = comps.replace(np.nan, \"\", regex=True)\n",
    "cols = comps.columns.drop('Sample')\n",
    "comps[cols] = comps[cols].apply(pd.to_numeric) # make columns numeric\n",
    "# add random number assignment\n",
    "rd = pd.read_excel('Z:\\\\Millennium Set\\\\Millennium_COMPS_viewonly.xlsx', usecols=[0,2])\n",
    "rd = rd.drop([0,1]).rename(columns={'DO NOT TOUCH THIS':'Sample',\n",
    "                                    'X.1':'rand_num'}).reset_index(drop=True)\n",
    "comps = pd.merge(rd, comps, how='right', on='Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c18bec",
   "metadata": {},
   "source": [
    "#### Datasets (baseline removal and normalization already applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f455d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_earth = pd.read_csv(fp+'CL_all_Earth_spectra.csv')\n",
    "cl_mars = pd.read_csv(fp+'CL_all_Mars_spectra.csv')\n",
    "cl_vac = pd.read_csv(fp+'CL_all_Vacuum_spectra.csv')\n",
    "cc_mars = pd.read_csv(fp+'CC_all_Mars_spectra.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f629b2",
   "metadata": {},
   "source": [
    "#### Sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b953a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivities = pd.read_csv(fp+'instrument_sensitivities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9567662",
   "metadata": {},
   "source": [
    "#### Split test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53287b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_250_1000 = comps[comps.rand_num >= 250].reset_index(drop=True)\n",
    "train_0_750 = comps[comps.rand_num <= 750].reset_index(drop=True)\n",
    "test_250_1000 = comps[comps.rand_num < 250].reset_index(drop=True)\n",
    "test_0_750 = comps[comps.rand_num > 750].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fde12",
   "metadata": {},
   "source": [
    "#### Outlier limits\n",
    "Calculated by 1.5*IQR + Q3 on entire MHC dataset or highest natural sample for doped elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ea24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_limits = pd.read_csv('Z:\\\\Millennium Set\\\\NEW_OUTLIER_LIMITS.csv')\n",
    "iqr_outliers = dict(zip(outlier_limits.element, outlier_limits.iqr_q3_outlier_limit))\n",
    "dope_outliers = dict(zip(outlier_limits.element, outlier_limits.highest_natural_for_doped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2afb60",
   "metadata": {},
   "source": [
    "#### Make models per element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f409b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = ['MnO', 'Na2O', 'SiO2', 'Li', 'Ni', 'Pb', 'Rb', 'Sr', 'Zn']\n",
    "n_ranges = ['0-750', '250-1000']\n",
    "factors = {\n",
    "'LOB' : 1.645,\n",
    "'LOD' : 3.3,\n",
    "'LOQ' : 10\n",
    "}\n",
    "methods = ['braga', 'metals']\n",
    "dfs = [cl_earth,cl_mars,cl_vac,cc_mars]\n",
    "df_names = ['CL_Earth', 'CL_Mars', 'CL_Vac', 'CC_Mars']\n",
    "mhc_list = [cl_earth,cl_mars,cl_vac]\n",
    "outliers = [iqr_outliers, dope_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "661a2a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddf07fa40144f61b15c5330b4a5cb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Number ranges:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Elements:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Elements:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLS parameters\n",
    "n_folds = 5\n",
    "max_components = 30\n",
    "\n",
    "# prep for results\n",
    "n_range_list = []\n",
    "element_list = []\n",
    "atm_list = []\n",
    "inst_list = []\n",
    "n_train_list = []\n",
    "rmsecv_list = []\n",
    "component_list = []\n",
    "rmsec_list = []\n",
    "train_r2_list = []\n",
    "train_adj_r2_list = []\n",
    "lob_list = []\n",
    "lod_list = []\n",
    "loq_list = []\n",
    "outlier_list = []\n",
    "\n",
    "for n_range in tqdm(n_ranges, desc='Number ranges'):\n",
    "\n",
    "    if n_range == '0-750':\n",
    "        all_train = train_0_750\n",
    "        all_test = test_0_750\n",
    "    else:\n",
    "        all_train = train_250_1000\n",
    "        all_test = test_250_1000\n",
    "\n",
    "\n",
    "    for element in tqdm(elements, leave=False, desc='Elements'):\n",
    "        count = 0\n",
    "\n",
    "        for df in tqdm(dfs, leave=False, desc='Dataset'):\n",
    "            \n",
    "            if df_names[count].split('_')[0]=='CC':\n",
    "                inst='LANL'\n",
    "            else:\n",
    "                inst='ChemLIBS'\n",
    "                \n",
    "            if df_names[count].split('_')[1]=='Vac':\n",
    "                atm = 'Vacuum'\n",
    "            else:\n",
    "                atm = df_names[count].split('_')[1]\n",
    "\n",
    "            outpath = \"{}\\\\python_models\\\\{}_{}\\\\\".format(fp, df_names[count], n_range)\n",
    "            \n",
    "            count +=1\n",
    "            \n",
    "            count1 = 0\n",
    "            \n",
    "            for outlier in outliers:\n",
    "                \n",
    "                if count1 == 0:\n",
    "                    o = 'iqr_q3'\n",
    "                else:\n",
    "                    o = 'highest_natural'\n",
    "                \n",
    "                count1 += 1\n",
    "                \n",
    "                out_lim = outlier[element]\n",
    "                \n",
    "                if out_lim.isna():\n",
    "                    temp_train = all_train[~all_train[element].isna()].reset_index(drop=True)[['Sample', element]]\n",
    "                else:\n",
    "                    temp_train = all_train[all_train[element] <= out_lim].reset_index(drop=True)[['Sample', element]]\n",
    "\n",
    "                # train metadata\n",
    "                train_names = sorted(set(temp_train.Sample).intersection(df.columns)) # sorted\n",
    "                y_train = temp_train[temp_train.Sample.isin(train_names)][element].values # already alphabetized\n",
    "                n_train = len(y_train)\n",
    "\n",
    "                # train spectra\n",
    "                X_train = df[train_names]\n",
    "                spec_list = []\n",
    "                for column in X_train.columns:\n",
    "                    spectrum = list(X_train[column])\n",
    "                    spec_list.append(spectrum)\n",
    "                X_train = np.array(spec_list)\n",
    "\n",
    "                # cross validation with PLS\n",
    "                cv_dict = {}\n",
    "\n",
    "                for n_components in np.arange(start=2, stop=max_components+1, step=1):\n",
    "                    # define model\n",
    "                    temp_pls = PLSRegression(n_components = n_components, scale=False)\n",
    "                    # run CV and get RMSE\n",
    "                    temp_rmsecv = (-cross_val_score(\n",
    "                        temp_pls, X_train, y_train, cv=n_folds, scoring='neg_root_mean_squared_error'\n",
    "                    )).mean()\n",
    "                    # add results to dictionary\n",
    "                    cv_dict.update({temp_rmsecv : n_components})\n",
    "\n",
    "                # select parameters of model with lowest rmsecv\n",
    "                rmsecv = min(list(cv_dict.keys()))\n",
    "                component = cv_dict[rmsecv]\n",
    "                model = PLSRegression(n_components = component, scale=False)\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                pickle.dump(model, open(outpath+element+'_model.asc', 'wb'), protocol=0)\n",
    "\n",
    "                coeff = pd.DataFrame(model.coef_)\n",
    "                coeff.to_csv(outpath+element+'_coeffs.csv', index=False)\n",
    "\n",
    "                for method in methods:\n",
    "\n",
    "                    sensitivity = sensitivities[\n",
    "                        (sensitivities.instrument == inst) &\n",
    "                        (sensitivities.atmosphere == atm) &\n",
    "                        (sensitivities.method == method)\n",
    "                    ]['sensitivity'].iloc[0]\n",
    "\n",
    "                    # LBDQ\n",
    "                    # calculate regression vector\n",
    "                    vector = pow(coeff, 2).sum().pow(.5)  #square root of sum of squares\n",
    "\n",
    "                    # calculate values\n",
    "                    lob = factors['LOB'] * sensitivity * vector[0]\n",
    "                    lod = factors['LOD'] * sensitivity * vector[0]\n",
    "                    loq = factors['LOQ'] * sensitivity * vector[0]\n",
    "                    \n",
    "                    # calibration error\n",
    "                    train_pred = model.predict(X_train)\n",
    "                    train_pred_true = pd.DataFrame({\n",
    "                        'sample' : train_names,\n",
    "                        'actual' : y_train.flatten().tolist(),\n",
    "                        'pred' : train_pred.flatten().tolist()\n",
    "                    })\n",
    "                    train_pred_true.loc[train_pred_true.pred < loq, ['pred']] = np.nan\n",
    "                    temp = train_pred_true.dropna()\n",
    "                    if len(temp) < 2:\n",
    "                        print('Fewer than 2 %s predictions above the LOQ of %s'%(element,loq))\n",
    "                        rmsec_list.append('NA')\n",
    "                        train_r2_list.append('NA')\n",
    "                        train_adj_r2_list.append('NA')\n",
    "                        continue\n",
    "\n",
    "                    rmsec = sqrt(mean_squared_error(temp.actual, temp.pred))\n",
    "                    train_r2 = model.score(X_train,y_train)\n",
    "                    train_adj_r2 = 1 - (1-train_r2)*(len(temp) - 1) / (len(temp) - (temp.shape[1] - 1) - 1)\n",
    "\n",
    "                    # fill with <LOQ\n",
    "                    train_pred_true.loc[train_pred_true.pred.isna(), ['pred']] = '<LOQ'\n",
    "                    train_pred_true.to_csv(outpath+element+\"_\"+n_range+'_train_preds.csv', index=False)\n",
    "\n",
    "                    # TEST MODEL\n",
    "                    \n",
    "                    n_range_list.append(n_range)\n",
    "                    outlier_list.append(o)\n",
    "                    method_list.append(method)\n",
    "                    element_list.append(element)\n",
    "                    atm_list.append(atm)\n",
    "                    inst_list.append(inst)\n",
    "                    n_train_list.append(n_train)\n",
    "                    rmsecv_list.append(rmsecv)\n",
    "                    component_list.append(component)\n",
    "                    lob_list.append(lob)\n",
    "                    lod_list.append(lod)\n",
    "                    loq_list.append(loq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "716ea465",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'element':element_list,\n",
    "    'outlier_defn':outlier_list,\n",
    "    'instrument':inst_list,\n",
    "    'atmosphere':atm_list,\n",
    "    'method':method_list,\n",
    "    'num_range':n_range_list,\n",
    "    'n_train':n_train_list,\n",
    "    'rmsecv':rmsecv_list,\n",
    "    'components':component_list,\n",
    "    'lob':lob_list,\n",
    "    'lod':lod_list,\n",
    "    'loq':loq_list\n",
    "})\n",
    "\n",
    "results.to_csv(fp+'train_results_011222.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09712f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
